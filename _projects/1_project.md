---
layout: page
title: "Computer Vision Research @ MIT Media Lab Personal Robotics Group"
description: For this research, I developed a multi-modal deep neural network for Dyadic Affect Analysis in Parent-child multi-modal interaction in order to enhance the performance of a speech-recognition-based model. I integrated yolov5 and DeepFace to differentiate parent versus child for accurate attention estimation (95% accuracy) and built pipeline to analyze emotions and nonverbal cues during dyadic interaction. I wrote one first-author paper "EmoLink: Facial and Emotion Perception System for Displaying Interpersonal Dynamics in Real-World Parent-Child Interactions" (submitted to ICMI 2024).

importance: 1

category: main
---

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/project_historical_reconstruction.png" title="" class="img-fluid rounded z-depth-1" %}
    </div>
</div>



